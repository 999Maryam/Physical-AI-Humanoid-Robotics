"use strict";(globalThis.webpackChunkai_textbook_frontend=globalThis.webpackChunkai_textbook_frontend||[]).push([[383],{6557:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"capstone","title":"Capstone","description":"This capstone chapter integrates the concepts learned throughout the textbook to build a simple, end-to-end AI-robot pipeline. It demonstrates how different components\u2014from perception and reasoning to action\u2014work together in a practical application.","source":"@site/docs/06.capstone-projects.mdx","sourceDirName":".","slug":"/capstone","permalink":"/Physical-AI-Humanoid-Robotics/docs/capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/06.capstone-projects.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"capstone","title":"Capstone","sidebar_label":"Capstone"},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action Systems","permalink":"/Physical-AI-Humanoid-Robotics/docs/vision-language-action-systems"}}');var i=t(4848),s=t(8453);const a={id:"capstone",title:"Capstone",sidebar_label:"Capstone"},r="Capstone",c={},l=[{value:"Project Overview",id:"project-overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone",children:"Capstone"})}),"\n",(0,i.jsx)(n.p,{children:"This capstone chapter integrates the concepts learned throughout the textbook to build a simple, end-to-end AI-robot pipeline. It demonstrates how different components\u2014from perception and reasoning to action\u2014work together in a practical application."}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"The goal of this capstone project is to create a robot system that can perform a basic task, such as identifying a specific object and moving it to a designated location, using a combination of vision, basic AI reasoning, and robotic control."}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Module"}),": Utilizes a camera sensor (simulated or real) and a simple object detection algorithm (e.g., color-based detection, basic machine learning model) to locate the target object."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reasoning Module"}),': A simple state machine or rule-based AI determines the next action based on the perceived environment and the task goal (e.g., "if object detected, move to grasp; if object grasped, move to drop-off location").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation Module"}),": Commands a robotic arm to execute grasping and placement actions. This involves inverse kinematics for reaching target poses and basic gripper control."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Communication"}),": ROS 2 will be used as the middleware to facilitate communication between the perception, reasoning, and manipulation nodes."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set up Simulation Environment"}),": Use Gazebo or Isaac Sim to create a virtual environment with a robot arm and target objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate Camera and Object Detection"}),": Develop or integrate a module for visual perception and object localization."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Develop Robotic Control"}),": Implement basic control for the robot arm, including inverse kinematics and gripper control."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create ROS 2 Nodes"}),": Package perception, reasoning, and manipulation logic into separate ROS 2 nodes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Define ROS 2 Topics/Services"}),": Establish communication channels for data exchange between nodes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Develop Task Logic"}),": Implement the main control loop that orchestrates the robot's behavior based on the task goal."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing and Refinement"}),": Thoroughly test the pipeline in simulation and debug any issues."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"Upon completing this capstone, you will have a practical understanding of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrating different AI and robotics components."}),"\n",(0,i.jsx)(n.li,{children:"Designing and implementing a complete robot system."}),"\n",(0,i.jsx)(n.li,{children:"Troubleshooting and debugging complex robotic pipelines."}),"\n",(0,i.jsx)(n.li,{children:"Applying theoretical knowledge to real-world (simulated) challenges."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);